{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Localization task ground-aerial"],"metadata":{"id":"lUdWs5mRQ1-o"}},{"cell_type":"markdown","source":["## IMPORTS"],"metadata":{"id":"xk9AGfP7QwHC"}},{"cell_type":"code","source":["import os\n","import cv2\n","import math\n","import torch\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm\n","from PIL import Image\n","from torchvision import transforms as T, models\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from torchvision.transforms.functional import to_pil_image\n"],"metadata":{"id":"LuXi2G7nLInf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## GLOBALS"],"metadata":{"id":"SSFqqtMuQs38"}},{"cell_type":"code","source":["SAVING = False\n","LOADING = False"],"metadata":{"id":"6b96kAdeSBM1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","print(\"PyTorch version:\", torch.__version__)\n","print(\"CUDA available:\", torch.cuda.is_available())\n","print(\"CUDA version:\", torch.version.cuda)\n","print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7LMoRTGiNeoF","executionInfo":{"status":"ok","timestamp":1752570080226,"user_tz":-120,"elapsed":26,"user":{"displayName":"Andrea LATTARO","userId":"04369168861908195949"}},"outputId":"179ff18d-c402-496f-ea05-2c71a54e91f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n","PyTorch version: 2.7.1+cu118\n","CUDA available: True\n","CUDA version: 11.8\n","Device: cuda\n"]}]},{"cell_type":"code","source":["path = \"C:/Users/andre/Desktop/vision_dataset/\"\n","csv_path_train = os.path.join(path, \"CVUSA_subset/training_csv.csv\")\n","csv_path_validation = os.path.join(path, \"CVUSA_subset/validation_csv.csv\")\n","csv_path_test = os.path.join(path, \"CVUSA_subset/test_csv.csv\")\n","\n","\n","train_df = pd.read_csv(csv_path_train, sep=\",\", names=[\n","     'ground', 'sintetic', 'segmentation', 'bingmap'\n"," ], encoding='utf-8', header=0)\n","\n","val_df = pd.read_csv(csv_path_validation, sep=\",\", names=[\n","     'ground', 'sintetic', 'segmentation', 'bingmap'\n"," ], encoding='utf-8', header=0)\n","\n","test_df = pd.read_csv(csv_path_test, sep=\",\", names=[\n","     'ground', 'sintetic', 'segmentation', 'bingmap'\n"," ], encoding='utf-8', header=0)"],"metadata":{"id":"t42gCU1PLU-o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## UTILS"],"metadata":{"id":"N2XcGXSSQjv0"}},{"cell_type":"code","source":["def recall_at_k(queries, database, K=1):\n","\n","    queries = F.normalize(queries, dim=1)\n","    database = F.normalize(database, dim=1)\n","\n","    dists = torch.cdist(queries, database, p=2)\n","\n","    _ , top_k = torch.topk(dists, k=K, largest=False)\n","\n","    correct = 0\n","    for i in range(len(queries)):\n","        if i in top_k[i]:\n","            correct += 1\n","\n","    return correct / len(queries), top_k\n","\n","\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","\n","def denormalize(tensor, mean, std):\n","    tensor = tensor.clone()\n","    for t, m, s in zip(tensor, mean, std):\n","        t.mul_(s).add_(m)\n","    return tensor.clamp(0, 1)\n","\n","\n","def show_top_k_images(queries, database, query_images, database_images, top_k, num_to_show=5):\n","    for i in range(num_to_show):\n","        fig, axs = plt.subplots(1, top_k.size(1) + 1, figsize=(16, 4))\n","\n","\n","        query_img = denormalize(query_images[i].cpu(), mean, std)\n","        axs[0].imshow(to_pil_image(query_img))\n","        axs[0].set_title(\"Query\")\n","        axs[0].axis(\"off\")\n","\n","        for j in range(top_k.size(1)):\n","            db_idx = top_k[i][j].item()\n","            db_img = denormalize(database_images[db_idx].cpu(), mean, std)\n","            axs[j+1].imshow(to_pil_image(db_img))\n","            axs[j+1].axis(\"off\")\n","            if db_idx == i:\n","                axs[j+1].set_title(f\"Top {j+1} []\")\n","            else:\n","                axs[j+1].set_title(f\"Top {j+1}\")\n","\n","        plt.tight_layout()\n","        plt.show()\n"],"metadata":{"id":"ExtDPTT3vbFY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## DATA"],"metadata":{"id":"hKlXFnwiO8vu"}},{"cell_type":"code","source":["transform = T.Compose([\n","    T.ToTensor(),\n","    T.Resize((256, 256)),\n","    T.Normalize(mean=[0.485, 0.456, 0.406],\n","                 std=[0.229, 0.224, 0.225])\n","\n","])\n","\n","\n","class CVUSATripletDataset(Dataset):\n","    def __init__(self, dataframe, root_dir, transform=None):\n","        self.df = dataframe.reset_index(drop=True)\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","        self.sv_images = [os.path.join(path, sv_filename) for sv_filename in self.df['ground'].to_list()]\n","        self.sat_images = [os.path.join(path, sat_filename) for sat_filename in self.df['bingmap'].to_list()]\n","        self.segmented = [os.path.join(path, depth_filename) for depth_filename in self.df['segmentation'].to_list()]\n","        self.syntetic = [os.path.join(path, depth_filename) for depth_filename in self.df['sintetic'].to_list()]\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","\n","        ground = cv2.cvtColor(cv2.imread(os.path.join(self.root_dir, self.sv_images[idx])) , cv2.COLOR_BGR2RGB)\n","        synthetic = cv2.cvtColor(cv2.imread(os.path.join(self.root_dir, self.syntetic[idx])) , cv2.COLOR_BGR2RGB)\n","\n","        segmented = cv2.imread(os.path.join(self.root_dir, self.segmented[idx]))\n","        segmented = cv2.cvtColor(segmented, cv2.COLOR_BGR2RGB)\n","        candidate_pos = cv2.cvtColor(cv2.imread(os.path.join(self.root_dir, self.sat_images[idx])) , cv2.COLOR_BGR2RGB)\n","\n","        neg_idx = random.choice([i for i in range(len(self.df)) if i != idx])\n","        candidate_neg = cv2.cvtColor(cv2.imread(os.path.join(self.root_dir, self.sat_images[neg_idx])) , cv2.COLOR_BGR2RGB)\n","\n","        if self.transform:\n","            ground = self.transform(ground)\n","            synthetic = self.transform(synthetic)\n","            candidate_pos = self.transform(candidate_pos)\n","            candidate_neg = self.transform(candidate_neg)\n","            segmented = self.transform(segmented)\n","\n","        return ground, synthetic, segmented, candidate_pos, candidate_neg\n","\n","    def _load_image(self, rel_path):\n","        img_path = os.path.join(self.root_dir, rel_path)\n","\n","        img = cv2.imread(img_path)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","        if self.transform:\n","            img = self.transform(img)\n","        return img\n","\n"],"metadata":{"id":"1c7_cB68O_QV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_set = CVUSATripletDataset(train_df, path, transform)\n","val_set = CVUSATripletDataset(val_df, path, transform)\n","test_set = CVUSATripletDataset(test_df, path, transform)\n","\n","batch_size = 8\n","train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"JSrpl3xkPKAy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## NETWORK"],"metadata":{"id":"gju63TP0OGOx"}},{"cell_type":"code","source":["class JointFeatureLearningNet(nn.Module):\n","    def __init__(self):\n","        super(JointFeatureLearningNet, self).__init__()\n","        self.ground_vgg = self._make_encoder()\n","        self.shared_vgg = self._make_encoder()\n","\n","        self.fusion = nn.Conv2d(4 * 512, 512, kernel_size=1, padding=1)\n","\n","    def _make_encoder(self):\n","        return models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features\n","\n","    def forward(self, ground, aerial, segmented, positive,negative):\n","        f_g = self.ground_vgg(ground)\n","        f_a = self.shared_vgg(aerial)\n","        f_s = self.shared_vgg(segmented)\n","        f_cp = self.shared_vgg(positive)\n","        f_cn = self.shared_vgg(negative)\n","\n","        fused = torch.cat([f_g, f_a, f_s, f_cp], dim=1)\n","\n","        joint_feat = self.fusion(fused)\n","\n","        return joint_feat , f_g , f_a , f_cp , f_cn\n","\n","class FeatureFusionNet(nn.Module):\n","    def __init__(self):\n","        super(FeatureFusionNet, self).__init__()\n","\n","        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fusion_fc = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(512 , 256)\n","        )\n","\n","    def forward(self,x):\n","        x = self.pool(x)\n","        x = self.fusion_fc(x)\n","        return x\n","\n","\n","class GeoLocalizationNet(nn.Module):\n","    def __init__(self):\n","        super(GeoLocalizationNet, self).__init__()\n","        self.joint_net = JointFeatureLearningNet()\n","        self.fusion_net = FeatureFusionNet()\n","\n","    def get_embedding(self, x):\n","        x = F.adaptive_avg_pool2d(x, (1, 1))\n","        return x.view(x.size(0), -1)\n","\n","    def forward(self, ground, synthetic, segmented, positive,negative):\n","        joint_feat,f_g,f_a,f_cp,f_cn = self.joint_net(ground, synthetic, segmented, positive,negative)\n","        f_cp = self.fusion_net(f_cp)\n","        f_cn = self.fusion_net(f_cn)\n","        f_a = self.fusion_net(f_a)\n","        f_g = self.fusion_net(f_g)\n","\n","        fused_feat = self.fusion_net(joint_feat)\n","\n","        return fused_feat,f_g,f_a,f_cp,f_cn"],"metadata":{"id":"Y9qXXK-YOBGh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def count_parameters(model):\n","    total_params = sum(p.numel() for p in model.parameters())\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    return total_params, trainable_params\n","\n","model = GeoLocalizationNet()\n","total_params, trainable_params = count_parameters(model)\n","print(f\"Total parameters: {total_params}\")\n","print(f\"Trainable parameters: {trainable_params}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nq6HtLIkVt-o","executionInfo":{"status":"ok","timestamp":1752570095247,"user_tz":-120,"elapsed":1045,"user":{"displayName":"Andrea LATTARO","userId":"04369168861908195949"}},"outputId":"c46ff882-334b-428d-df8f-d956d17282a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total parameters: 30609792\n","Trainable parameters: 30609792\n"]}]},{"cell_type":"markdown","source":["### Loss function"],"metadata":{"id":"vM2zYGVaOjue"}},{"cell_type":"code","source":["class WeightedSoftMarginTripletLoss(nn.Module):\n","      def __init__(self, alpha=0.1, lambda1=1.0, lambda2=1.0, reduction='mean'):\n","          super().__init__()\n","          self.alpha = alpha\n","          self.lambda1 = lambda1\n","          self.lambda2 = lambda2\n","          self.reduction = reduction\n","      def forward(self, fg, fa_pos, fa_neg, fa_syn):\n","\n","          dp = F.pairwise_distance(fg, fa_pos, p=2)\n","          dn = F.pairwise_distance(fg, fa_neg, p=2)\n","          ds = F.pairwise_distance(fa_syn, fa_pos, p=2)\n","\n","          triplet_term = torch.log1p(torch.exp(self.alpha * (dp - dn)))\n","          aux_term = torch.log1p(torch.exp(self.alpha * ds))\n","\n","          loss = self.lambda1 * triplet_term + self.lambda2 * aux_term\n","\n","          if self.reduction == 'mean':\n","              return loss.mean()\n","          else:\n","              return loss"],"metadata":{"id":"LZLGrs2KOsfg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## TRAIN"],"metadata":{"id":"M19VJjuqMr_a"}},{"cell_type":"code","source":["# === Training ===\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = GeoLocalizationNet().to(device)\n","criterion = WeightedSoftMarginTripletLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","\n","checkpoint_path = 'C:/Users/andre/Desktop/vision_dataset/checkpoint_parte2_alpha10/checkpoint8.pth'\n","\n","if LOADING:\n","    start_epoch = 8\n","    checkpoint = torch.load(checkpoint_path, map_location=device)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    start_epoch = checkpoint['epoch']\n","    print(f\"Checkpoint loaded. Epoch: {start_epoch}\")\n","else:\n","    start_epoch = 0\n","\n","i=0\n","n_epochs = 30\n","for epoch in range(start_epoch,n_epochs):\n","    model.train()\n","    total_loss = 0\n","    for ground, synthetic, segmented, pos, neg in tqdm(train_loader):\n","        ground, synthetic = ground.to(device), synthetic.to(device)\n","        segmented, pos, neg = segmented.to(device), pos.to(device), neg.to(device)\n","        optimizer.zero_grad()\n","        output,anchor,synthetic,positive,negative = model(ground, synthetic, segmented, pos , neg)\n","        negative = negative.detach()\n","        i=i+1\n","        loss = criterion(output,positive,negative,synthetic)\n","\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()/ground.shape[0]\n","\n","    avg_loss = total_loss / (int(len(train_loader)/batch_size))\n","\n","    print(f\"Epoch {epoch+1}/{n_epochs} - Train Loss: {avg_loss:.4f}\")\n","\n","\n","    # === Validation ===\n","\n","    model.eval()\n","    embeddings = []\n","    positive_embeddings = []\n","    query_imgs_list = []\n","    database_imgs_list = []\n","    val_loss = 0\n","\n","    with torch.no_grad():\n","        for ground, synthetic, segmented, pos, neg in tqdm(val_loader):\n","            ground, synthetic = ground.to(device), synthetic.to(device)\n","            segmented, pos, neg = segmented.to(device), pos.to(device), neg.to(device)\n","\n","            fused_feat, f_g, f_a, f_cp, f_cn = model(ground, synthetic, segmented, pos, neg)\n","\n","            loss = criterion(fused_feat, f_cp, f_cn, f_a)\n","            val_loss += loss.item()/ground.shape[0]\n","\n","            embeddings.append(fused_feat.cpu())\n","            positive_embeddings.append(f_cp.cpu())\n","\n","            query_imgs_list.append(ground.cpu())\n","            database_imgs_list.append(pos.cpu())\n","\n","    avg_val_loss = val_loss / (int(len(val_loader)/batch_size))\n","    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n","\n","    all_queries = torch.cat(embeddings, dim=0)\n","    all_database = torch.cat(positive_embeddings, dim=0)\n","    query_images = torch.cat(query_imgs_list, dim=0)\n","    database_images = torch.cat(database_imgs_list, dim=0)\n","\n","    recall_1 , top_k1 = recall_at_k(all_queries, all_database, K=1)\n","    recall_5 , top_k5 = recall_at_k(all_queries, all_database, K=5)\n","    recall_10 , top_k10 = recall_at_k(all_queries, all_database, K=10)\n","\n","    print(f\"Recall@1: {recall_1:.4f}, Recall@5: {recall_5:.4f} , Recall@10: {recall_10:.4f}\")\n","\n","    show_top_k_images(all_queries, all_database, query_images, database_images, top_k5, num_to_show=5)\n","\n","    if SAVING:\n","      checkpoint = {\n","              'epoch': epoch + 1,\n","              'model_state_dict': model.state_dict(),\n","              'optimizer_state_dict': optimizer.state_dict(),\n","              'train_loss': avg_loss,\n","              'val_loss': avg_val_loss,\n","              'recall_1': recall_1,\n","              'recall_5': recall_5,\n","              'recall_10': recall_10\n","          }\n","      torch.save(checkpoint, f'C:/Users/andre/Desktop/vision_dataset/checkpoint_parte2_alpha10/checkpoint{epoch}.pth')"],"metadata":{"id":"f8y1BuVtWPYu","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"144z-TdHq2_McYFvFnhD0-GG9udWhUyCU"},"outputId":"d7ee807b-e705-4b72-f644-87a9ec6f10ec","executionInfo":{"status":"error","timestamp":1752318542146,"user_tz":-120,"elapsed":7675403,"user":{"displayName":"Andrea LATTARO","userId":"04369168861908195949"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## TEST\n"],"metadata":{"id":"ylkG1g3Kvy24"}},{"cell_type":"code","source":["checkpoint_path = 'C:/Users/andre/Desktop/vision_dataset/checkpoint_parte2_alpha10/Geolocalization_best_epoch.pth'\n","model = GeoLocalizationNet().to(device)\n","\n","checkpoint = torch.load(checkpoint_path, map_location=device)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","\n","start_epoch = checkpoint['epoch']\n","print(f\"Checkpoint loaded. Epoch: {start_epoch}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PI3rCWz_SvcU","executionInfo":{"status":"ok","timestamp":1752570132765,"user_tz":-120,"elapsed":1293,"user":{"displayName":"Andrea LATTARO","userId":"04369168861908195949"}},"outputId":"8d8c4b49-6c59-4a6b-9897-bd980b1baf47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Checkpoint loaded. Epoch: 21\n"]}]},{"cell_type":"code","source":["model.eval()\n","embeddings = []\n","positive_embeddings = []\n","query_imgs_list = []\n","database_imgs_list = []\n","\n","with torch.no_grad():\n","    for ground, synthetic, segmented, pos, neg in tqdm(test_loader):\n","        ground, synthetic = ground.to(device), synthetic.to(device)\n","        segmented, pos, neg = segmented.to(device), pos.to(device), neg.to(device)\n","\n","        fused_feat, f_g, f_a, f_cp, f_cn = model(ground, synthetic, segmented, pos, neg)\n","\n","        embeddings.append(fused_feat.cpu())\n","        positive_embeddings.append(f_cp.cpu())\n","\n","        query_imgs_list.append(ground.cpu())\n","        database_imgs_list.append(pos.cpu())\n","\n","all_queries = torch.cat(embeddings, dim=0)\n","all_database = torch.cat(positive_embeddings, dim=0)\n","query_images = torch.cat(query_imgs_list, dim=0)\n","database_images = torch.cat(database_imgs_list, dim=0)\n","\n","recall_1 , top_k1 = recall_at_k(all_queries, all_database, K=1)\n","recall_5 , top_k5 = recall_at_k(all_queries, all_database, K=5)\n","recall_10 , top_k10 = recall_at_k(all_queries, all_database, K=10)\n","\n","print(f\"Recall@1: {recall_1:.4f}, Recall@5: {recall_5:.4f} , Recall@10: {recall_10:.4f}\")\n","\n","show_top_k_images(all_queries, all_database, query_images, database_images, top_k5, num_to_show=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1HWItql7GQMfJ0Ais594QOCN-we_L1E2T"},"id":"DKTR6X76TyJK","executionInfo":{"status":"ok","timestamp":1752570196722,"user_tz":-120,"elapsed":61904,"user":{"displayName":"Andrea LATTARO","userId":"04369168861908195949"}},"outputId":"861d367c-f064-40e2-be3b-4c0c5bff97bb"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}